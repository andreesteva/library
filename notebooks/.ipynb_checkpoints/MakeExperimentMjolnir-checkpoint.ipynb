{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Esc + II to interupt kernel\n",
    "# ESC + 00 to restart kernel\n",
    "import json\n",
    "import os\n",
    "import lib\n",
    "from lib.taxonomy.utils import SynonymsList\n",
    "from lib.notebooks.vis_utils import tic, toc\n",
    "from lib.taxonomy.loading import getEntryValues, gatherSynset, gatherPathsAndLabels\n",
    "from lib.taxonomy.loading import TRAINING_SET, TESTING_SET, NO_SET, VALIDATION_SET\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_directory = '/archive/esteva/skindata4/images/'\n",
    "meta_file = '/archive/esteva/skindata4/meta.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def imageExists(m, directory):\n",
    "    \"\"\"Returns true if the image pointed to by data_point exists on the filesystem.\n",
    "\n",
    "        Args:\n",
    "            data_point(dict): A dictionary in 'skindata' format.\n",
    "            directory (str): A path to the directory where we check if the image exists.\n",
    "\n",
    "        Returns:\n",
    "            True if the image exists, false otherwise.\n",
    "    \"\"\"\n",
    "    p = os.path.join(directory, m['filename'])\n",
    "    return os.path.exists(p)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_all = json.load(open(meta_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Meta Entries: 300289\n",
      "Kept Meta Entries: 135393\n"
     ]
    }
   ],
   "source": [
    "# We load in images that exist on our filesystem\n",
    "skin_prob = 0.4\n",
    "tax_path_score = 0.8\n",
    "\n",
    "meta = meta_all\n",
    "meta = [m for m in meta if imageExists(m, dataset_directory)]\n",
    "meta_exists = meta\n",
    "meta = [m for m in meta if 'tax_path_score' in m.keys() and m['tax_path_score'] >= tax_path_score]\n",
    "meta = [m for m in meta if m['tax_path']]\n",
    "meta = [m for m in meta if 'skin_prob' in m.keys() and m['skin_prob'] >= skin_prob]\n",
    "\n",
    "# Fix the naming convention issues of the top 9 categories\n",
    "syns = SynonymsList()\n",
    "for m in meta:\n",
    "    rootname = '-'.join(m['tax_path'][0])\n",
    "    rootrename = syns.synonymOf(rootname).split('-')\n",
    "    m['tax_path'][0] = rootrename\n",
    "\n",
    "    \n",
    "# Rename 'label' field to 'disease_name'\n",
    "for m in meta:\n",
    "    if 'label' in m:\n",
    "        m['disease_name'] = m['label']\n",
    "        m['label'] = None\n",
    "    \n",
    "print \"Total Meta Entries: %d\" % len(meta_all)\n",
    "print \"Kept Meta Entries: %d\" % len(meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 cutaneous-lymphoma\n",
      "1 dermal-tumor-benign\n",
      "2 dermal-tumor-malignant\n",
      "3 epidermal-tumor-benign\n",
      "4 epidermal-tumor-malignant\n",
      "5 genodermatosis\n",
      "6 inflammatory\n",
      "7 pigmented-lesion-benign\n",
      "8 pigmented-lesion-malignant\n"
     ]
    }
   ],
   "source": [
    "def rootNodeClasses(meta):\n",
    "    \"\"\"Creates labels for our dataset based on the root nodes (the 9).\n",
    "\n",
    "        Args:\n",
    "            meta(list): a list of dictionaries in 'skindata' format.\n",
    "\n",
    "        Returns:\n",
    "            A dictionary of [label : class_name] key-value pairs sorted alphabetically.\n",
    "            A list of integer labels for each entry in meta.\n",
    "    \"\"\"\n",
    "    root_nodes = {rootNode(m) for m in meta}\n",
    "    root_nodes = np.sort([r for r in root_nodes])\n",
    "    root_nodes = {r : i for i,r in enumerate(root_nodes)}\n",
    "    labels = [root_nodes[rootNode(entry)] for entry in meta]\n",
    "    classes = {v : k for k,v in root_nodes.iteritems()}\n",
    "    return classes, np.array(labels)\n",
    "\n",
    "\n",
    "def rootNode(data_point):\n",
    "    \"\"\"Return the root node in the taxonomy for a data point.\n",
    "\n",
    "        Args:\n",
    "            data_point(dict): a dict in skindata format.\n",
    "        Returns:\n",
    "            The name of the root node.\n",
    "    \"\"\"\n",
    "    return '-'.join(data_point['tax_path'][0])\n",
    "\n",
    "def setEntries(meta, key, values):\n",
    "    \"\"\"Set the key in each entry of meta to its corresponding entry in the list value\n",
    "\n",
    "        Args:\n",
    "            meta (list): A list of dictionaries in 'skindata' format.\n",
    "            key (string): the key to add/alter in each dictionary of meta.\n",
    "            values (iterable, or single value): the iterable of values to add. len(values) == len(meta).\n",
    "                If values is not iterable (i.e. a single value) then we set all entries of meta[key] to this value.\n",
    "    \"\"\"\n",
    "    # If Values is not iterable\n",
    "    if not hasattr(values, '__iter__'):\n",
    "        values = len(meta) * [values]\n",
    "\n",
    "    assert(len(meta) == len(values))\n",
    "    for m, v in zip(meta, values):\n",
    "        m[key] = v\n",
    "\n",
    "classes, labels = rootNodeClasses(meta)\n",
    "setEntries(meta, 'label', labels)\n",
    "setEntries(meta, 'clinical_label', labels)\n",
    "for k,v in classes.iteritems():\n",
    "    print k,v\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FUNC: insert_datetime_field] Skipping 172 entries that could not load datetime\n",
      "46288 Entries have the datetime metadata\n",
      "Calculating time-camera edge matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/numpy/core/fromnumeric.py:2652: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.\n",
      "  VisibleDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000 / 43591 Elapsed Time: 5.56227016449 Time Remaining: 0.499201267231 Elapsed Time:  100.792287827\n"
     ]
    }
   ],
   "source": [
    "# Calculate time-camera edge matrix as a sparse matrix (about 2 minutes)\n",
    "from lib.taxonomy.edge_extraction import *\n",
    "\n",
    "meta = meta_exists\n",
    "for i,m in enumerate(meta):\n",
    "    m['index'] = i\n",
    "    \n",
    "insert_datetime_field(meta)\n",
    "insert_abs_time_field(meta)\n",
    "\n",
    "meta_datetime = getEntries(meta, 'datetime', None)\n",
    "cameras, camera_models = extract_cameras(meta_datetime)\n",
    "abs_times = np.array([m['abs_time'] for m in meta_datetime]).reshape((-1,1))\n",
    "cam_indx = np.array([camera_models[c] for c in cameras]).reshape((-1,1))\n",
    "print '%d Entries have the datetime metadata' % len(meta_datetime)\n",
    "\n",
    "print 'Calculating time-camera edge matrix...'\n",
    "M = len(meta_datetime)\n",
    "N = len(meta)\n",
    "\n",
    "t = tic()\n",
    "edge, _, _= edge_matrix(abs_times, cam_indx)\n",
    "edge = squareform_sparse(edge, M)\n",
    "toc(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the N x N Edge matrix\n",
    "# (will add edges to it, below)\n",
    "\n",
    "import scipy.sparse as sp\n",
    "E = sp.lil_matrix((N,N), dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 43591 edges to the graph\n"
     ]
    }
   ],
   "source": [
    "# Insert the datetime edges\n",
    "c = 0\n",
    "for i,j,v in sparse_matrix_iterator(edge):\n",
    "    if v:\n",
    "        c += 1\n",
    "        idx_i = meta_datetime[i]['index']\n",
    "        idx_j = meta_datetime[j]['index']\n",
    "        E[idx_i, idx_j] = v   \n",
    "print 'Adding %d edges to the graph' % c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 5183 edges to the graph\n",
      "Adding 2647 edges to the graph\n"
     ]
    }
   ],
   "source": [
    "# Add into the edge matrix the duplicates of turk2 and turk 1\n",
    "turk_results = [\n",
    "    '/archive/esteva/skindata4/duplicate_urls_turk1.json',\n",
    "    '/archive/esteva/skindata4/duplicate_urls_turk2.json',    \n",
    "    ]\n",
    "\n",
    "def dict2list(dict_):\n",
    "    list_ = []\n",
    "    for key, value in dict_.iteritems():\n",
    "        d = [key]\n",
    "        d.extend([v for v in value])\n",
    "        list_.append(d)   \n",
    "    return list_\n",
    "\n",
    "for tr in turk_results:\n",
    "    turk = json.load(open(tr, 'r'))\n",
    "    duplicates = dict2list(turk)\n",
    "    insert_edges_into_edge_matrix(E, duplicates, meta, field_name='filename')\n",
    "    print 'Adding %d edges to the graph' % np.sum([len(v)-1 for v in duplicates])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 17204 edges to the graph\n"
     ]
    }
   ],
   "source": [
    "# Add dermquest ids into the graph.\n",
    "dermquest = getEntries(meta, 'database', 'dermquest')\n",
    "dermquest = getEntries(dermquest, 'case', None)\n",
    "case2meta = Field2meta(dermquest, field='case')\n",
    "cases = np.unique(getEntryValues(dermquest, 'case'))\n",
    "duplicates = [[m['index'] for m in case2meta(case)] for case in cases]\n",
    "insert_edges_into_edge_matrix(E, duplicates, meta, field_name='index')\n",
    "print 'Adding %d edges to the graph' % np.sum([len(v)-1 for v in duplicates])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 21434 edges to the graph\n"
     ]
    }
   ],
   "source": [
    "# Add meta entries that share the same filenames as edges\n",
    "filename2meta = Field2meta(meta, field='filename')\n",
    "filenames = np.unique([m['filename'] for m in meta])\n",
    "duplicates = []\n",
    "for fn in filenames:\n",
    "    meta_filename = filename2meta(fn)\n",
    "    if len(meta_filename) == 0:\n",
    "        print 'wtf'\n",
    "        break\n",
    "    if len(meta_filename) > 1:\n",
    "        duplicates.append([m['index'] for m in meta_filename])\n",
    "insert_edges_into_edge_matrix(E, duplicates, meta, field_name='index')\n",
    "print 'Adding %d edges to the graph' % np.sum([len(v)-1 for v in duplicates])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We find 238254 connected components\n"
     ]
    }
   ],
   "source": [
    "# Extract connected components and assign them to the meta\n",
    "n_components, connected_components = sp.csgraph.connected_components(E, directed=False)\n",
    "unique_component_numbers, component_sizes = np.unique(connected_components, return_counts=True)\n",
    "\n",
    "for m, c in zip(meta, connected_components):\n",
    "    m['connected_component'] = c\n",
    "print 'We find %d connected components' % n_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proposed Test Set has 20958 entries\n"
     ]
    }
   ],
   "source": [
    "# Propose a test set (from the turked set)\n",
    "test_set = '/archive/esteva/skindata4/duplicate_urls_turk2.json'\n",
    "test_set = json.load(open(test_set, 'r'))\n",
    "test_set = [key for key in test_set.keys()]\n",
    "\n",
    "filename2meta = Field2meta(meta, field='filename')\n",
    "cc2meta = Field2meta(meta, field='connected_component')\n",
    "\n",
    "meta_test = [m for fn in test_set for m in filename2meta(fn)]\n",
    "setEntries(meta, 'set_identifier', TRAINING_SET)\n",
    "setEntries(meta_test, 'set_identifier', TESTING_SET)\n",
    "\n",
    "print 'Proposed Test Set has %d entries' % len(meta_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitioned Test Set has 19644 meta entries\n"
     ]
    }
   ],
   "source": [
    "# Iterate over elements of the test set and push connected components to train or test\n",
    "\n",
    "def component_is_split(comp):\n",
    "    \"\"\"Returns true if this component is split between train and test\"\"\"\n",
    "    set_ids = set([m['set_identifier'] for m in comp if m['set_identifier'] != NO_SET])    \n",
    "    return len(set_ids) == 2\n",
    "\n",
    "def random_partition(meta_test, return_splits=False):\n",
    "    \"\"\"Randomly partitions the components of meta_test to train/test by flipping a coin.\"\"\"\n",
    "    split_comps = []\n",
    "    for m in meta_test:\n",
    "        comp = cc2meta(m['connected_component'])    \n",
    "        if component_is_split(comp):\n",
    "            if np.random.rand() < 0.5:\n",
    "                setEntries(comp, 'set_identifier', TRAINING_SET)\n",
    "            else:\n",
    "                setEntries(comp, 'set_identifier', TESTING_SET)\n",
    "            split_comps.append(comp)\n",
    "    if return_splits:\n",
    "        return split_comps        \n",
    "    \n",
    "def maxset_partition(meta_test):\n",
    "    \"\"\"Deterministically place component into whichever set they already have more images of.\"\"\"\n",
    "    for m in meta_test:\n",
    "        comp = cc2meta(m['connected_component'])    \n",
    "        if component_is_split(comp):\n",
    "            N_test = len(getEntries(comp, 'set_identifier', TESTING_SET))\n",
    "            N_train = len(getEntries(comp, 'set_identifier', TRAINING_SET))\n",
    "            if N_test >= N_train:\n",
    "                setEntries(comp, 'set_identifier', TESTING_SET)\n",
    "            else:\n",
    "                setEntries(comp, 'set_identifier', TRAINING_SET)        \n",
    "\n",
    "maxset_partition(meta_test)\n",
    "meta_test = getEntries(meta, 'set_identifier', TESTING_SET)\n",
    "\n",
    "print 'Partitioned Test Set has %d meta entries' % len(meta_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning out 157244 / 292637 meta entries to NO_SET and assigning labels and clinical labels\n",
      "Final size of meta_train 117293\n",
      "Final size of meta_test 18100\n"
     ]
    }
   ],
   "source": [
    "# Assign NO_SET to meta entries that dont make the threshold, and reduce meta to just those that do.\n",
    "# We both assigning NO_SET so that meta_all is a reflection of meta but with the extra entries as NO_SET\n",
    "\n",
    "meta = meta_exists\n",
    "pruned = 0\n",
    "for m in meta:\n",
    "    if 'skin_prob' not in m or 'tax_path_score' not in m or 'tax_path' not in m:\n",
    "        m['set_identifier'] = NO_SET\n",
    "        pruned += 1\n",
    "        continue\n",
    "    if m['skin_prob'] < skin_prob or m['tax_path_score'] < tax_path_score:\n",
    "        m['set_identifier'] = NO_SET\n",
    "        pruned += 1\n",
    "        \n",
    "meta = [m for m in meta if m['set_identifier'] in [TRAINING_SET, TESTING_SET]]\n",
    "classes, labels = rootNodeClasses(meta)\n",
    "setEntries(meta, 'label', labels)\n",
    "setEntries(meta, 'clinical_label', labels)\n",
    "\n",
    "meta_train = getEntries(meta, 'set_identifier', TRAINING_SET)\n",
    "meta_test = getEntries(meta, 'set_identifier', TESTING_SET)\n",
    "synset = gatherSynset(meta_train)\n",
    "\n",
    "print 'Pruning out %d / %d meta entries to NO_SET and assigning labels and clinical labels' % (pruned, len(meta_exists))\n",
    "print 'Size of meta_train %d' % len(meta_train)\n",
    "print 'Size of meta_test %d' % len(meta_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14839\n"
     ]
    }
   ],
   "source": [
    "# Keep only the test set entries that have passed manual curation\n",
    "curated_test = [line.strip() for line in \n",
    "                open('/archive/esteva/skindata4/splits/nine-way/test_curated.txt').readlines()]\n",
    "curated_test = np.array([os.path.basename(t.split()[0]) for t in curated_test])\n",
    "    \n",
    "filename2meta = Field2meta(meta_test, field='filename')\n",
    "for fn in curated_test:\n",
    "    ms = filename2meta(fn)\n",
    "    for m in ms:\n",
    "        m['cc_keep'] = True\n",
    "    \n",
    "for m in meta_test:\n",
    "    if 'cc_keep' not in m:\n",
    "        m['set_identifier'] = NO_SET\n",
    "        \n",
    "meta_test = getEntries(meta, 'set_identifier', TESTING_SET)\n",
    "print len(meta_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Training, Validation, and Testing sets.\n",
      "Train and test share 0 images, according to filenames\n",
      "Dataset sizes (Based on Metadata):\n",
      "Train,\tVal,\tTest,\tTotal\n",
      "1036 \t0 \t118 \t1154\n",
      "7919 \t0 \t872 \t8791\n",
      "950 \t0 \t76 \t1026\n",
      "4567 \t0 \t641 \t5208\n",
      "8808 \t0 \t1461 \t10269\n",
      "5085 \t0 \t368 \t5453\n",
      "81907 \t0 \t10740 \t92647\n",
      "2777 \t0 \t380 \t3157\n",
      "4244 \t0 \t183 \t4427\n",
      "\n",
      "117293 0 14839\n",
      "\n",
      "Dataset sizes (Based on unique images):\n",
      "Train,\tVal,\tTest,\tTotal\n",
      "978 \t0 \t117 \t1095\n",
      "7618 \t0 \t858 \t8476\n",
      "925 \t0 \t76 \t1001\n",
      "4426 \t0 \t635 \t5061\n",
      "8443 \t0 \t1449 \t9892\n",
      "4959 \t0 \t366 \t5325\n",
      "77165 \t0 \t10666 \t87831\n",
      "2699 \t0 \t375 \t3074\n",
      "4161 \t0 \t170 \t4331\n",
      "# Unique Images in Training: 111374\n",
      "# Unique Images in Validation: 0\n",
      "# Unique Images in Testing: 14712\n",
      "# Unique Images tossed out: 3245\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print 'Generating Training, Validation, and Testing sets.'\n",
    "\n",
    "# Gather each dataset's paths and labels\n",
    "trainset = gatherPathsAndLabels(meta, dataset_directory, TRAINING_SET)\n",
    "valset = gatherPathsAndLabels(meta, dataset_directory, VALIDATION_SET)\n",
    "testset = gatherPathsAndLabels(meta, dataset_directory, TESTING_SET)\n",
    "no_set = gatherPathsAndLabels(meta, dataset_directory, NO_SET)\n",
    "\n",
    "# Since some images have multiple diseases, we keep only the unique 'path [label]' entries\n",
    "trainset = np.unique(trainset)\n",
    "valset = np.unique(valset)\n",
    "testset = np.unique(testset)\n",
    "noset = np.unique(no_set)\n",
    "\n",
    "# Lets check that there is no overlap between train and test paths\n",
    "trainpaths = np.unique([os.path.basename(t.split()[0]) for t in trainset])\n",
    "testpaths = np.unique([os.path.basename(t.split()[0]) for t in testset])\n",
    "intersection = np.intersect1d(trainpaths, testpaths)\n",
    "print 'Train and test share %d images, according to filenames' % len(intersection)\n",
    "\n",
    "getClassFromValidationSet = lambda meta, c: [m for m in meta if m['set_identifier'] == VALIDATION_SET and m['clinical_label'] == c]\n",
    "getClassFromTrainingSet = lambda meta, c: [m for m in meta if m['set_identifier'] == TRAINING_SET and m['clinical_label'] == c]\n",
    "getClassFromTestingSet = lambda meta, c: [m for m in meta if m['set_identifier'] == TESTING_SET and m['clinical_label'] == c]\n",
    "print 'Dataset sizes (Based on Metadata):'\n",
    "print 'Train,\\tVal,\\tTest,\\tTotal'\n",
    "for c in classes:\n",
    "    v = len(getClassFromValidationSet(meta, c))\n",
    "    t = len(getClassFromTrainingSet(meta, c))\n",
    "    te = len(getClassFromTestingSet(meta, c))\n",
    "    print t, '\\t', v, '\\t', te, '\\t', v + t + te\n",
    "\n",
    "print ''\n",
    "print len(getEntries(meta, 'set_identifier', TRAINING_SET)),\n",
    "print len(getEntries(meta, 'set_identifier', VALIDATION_SET)),\n",
    "print len(getEntries(meta, 'set_identifier', TESTING_SET))\n",
    "print ''\n",
    "\n",
    "print 'Dataset sizes (Based on unique images):'\n",
    "print 'Train,\\tVal,\\tTest,\\tTotal'\n",
    "for c in classes:    \n",
    "    v = len(np.unique([m['filename'] for m in getClassFromValidationSet(meta, c)]))\n",
    "    t = len(np.unique([m['filename'] for m in getClassFromTrainingSet(meta, c)]))\n",
    "    te = len(np.unique([m['filename'] for m in getClassFromTestingSet(meta, c)]))\n",
    "    print t, '\\t', v, '\\t', te, '\\t', v + t + te\n",
    "    \n",
    "print '# Unique Images in Training:', len(trainset)\n",
    "print '# Unique Images in Validation:', len(valset)\n",
    "print '# Unique Images in Testing:', len(testset)\n",
    "print '# Unique Images tossed out:', len(noset)\n",
    "print ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ./train\n",
      "Creating ./val\n"
     ]
    }
   ],
   "source": [
    "# Make training directory structure\n",
    "def make_directory_structure(dir_name, subfolders):\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "    for s in subfolders:\n",
    "        p = os.path.join(dir_name, s)\n",
    "        os.makedirs(p)\n",
    "    print 'Creating %s' % dir_name\n",
    "    \n",
    "subclasses = [s.split()[1] for s in synset]\n",
    "make_directory_structure('./train', subclasses)\n",
    "make_directory_structure('./val', subclasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_symlinks(dataset, dirname, subclasses):\n",
    "    syms = []\n",
    "    for entry in dataset:\n",
    "        p = entry.split()[0]\n",
    "        l = int(entry.split()[1])\n",
    "        s = \" \".join([p, os.path.join(dirname, subclasses[l], os.path.basename(p))])\n",
    "        syms.append(s)\n",
    "    return syms\n",
    "\n",
    "syms_train = generate_symlinks(trainset, './train', subclasses)\n",
    "syms_test = generate_symlinks(testset, './test', subclasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_symlinks(symlinks):\n",
    "    for entry in symlinks:\n",
    "        src = entry.split()[0]\n",
    "        dst = entry.split()[1]\n",
    "        os.symlink(src, dst)\n",
    "        \n",
    "create_symlinks(syms_train)\n",
    "create_symlinks(syms_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
