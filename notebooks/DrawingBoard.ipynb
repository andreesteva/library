{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Skin net to protobuf for use in freeze_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "\n",
    "import os.path\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from inception import inception_model\n",
    "\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_string('export_dir', '/tmp/skin_inception_export',\n",
    "                           \"\"\"Directory where to export inference model protobuf.\"\"\")\n",
    "tf.app.flags.DEFINE_string('graph_name', 'graph.pb',\n",
    "                           \"\"\"Filename of inference model protobuf.\"\"\")\n",
    "\n",
    "tf.app.flags.DEFINE_integer('image_size', 299,\n",
    "                            \"\"\"Needs to provide same value as in training.\"\"\")\n",
    "tf.app.flags.DEFINE_string('checkpoint_dir', '',\n",
    "                           \"\"\"Directory where to read training checkpoints.\"\"\")\n",
    "\n",
    "SYNSET_FILE = '/media/esteva/ExtraDrive1/ThrunResearch/data/skindata4/splits/nine-way/labels.txt'\n",
    "NUM_CLASSES = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FLAGS.checkpoint_dir = '/media/esteva/ExtraDrive1/ThrunResearch/tf_experiments/nine-way/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def inference_input():\n",
    "    \"\"\"Returns ops that convert raw image data to a 4D tensor representing a single image.\n",
    "    \n",
    "    Taken from:\n",
    "    https://github.com/tensorflow/serving/blob/master/tensorflow_serving/example/inception_export.py\n",
    "    \n",
    "    The input to the first op can be read using:\n",
    "    tf.gfile.FastGFile(image_filename, 'r').read()\n",
    "    \n",
    "    \"\"\"\n",
    "    jpegs = tf.placeholder(tf.string, shape=(1), name='input')\n",
    "    image_buffer = tf.squeeze(jpegs, [0])\n",
    "    image = tf.image.decode_jpeg(image_buffer, channels=3)\n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "    image = tf.image.central_crop(image, central_fraction=0.875)\n",
    "    image = tf.expand_dims(image, 0)\n",
    "    image = tf.image.resize_bilinear(image, [FLAGS.image_size, FLAGS.image_size], align_corners=False)\n",
    "    image = tf.squeeze(image, [0])    \n",
    "    image = tf.sub(image, 0.5)\n",
    "    image = tf.mul(image, 2.0)\n",
    "    images = tf.expand_dims(image, 0)\n",
    "    \n",
    "    return images, jpegs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully loaded model from model.ckpt-90000 at step=90000.\n",
      "Successfully converted checkpoint /media/esteva/ExtraDrive1/ThrunResearch/tf_experiments/nine-way/train into proto /tmp/skin_inception_export/graph.pb with inputs of size 299\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default(), tf.Session() as sess:      \n",
    "    input_, image_raw = inference_input()    \n",
    "    \n",
    "    logits, _ = inception_model.inference(input_, NUM_CLASSES + 1)\n",
    "    softmax = tf.nn.softmax(logits, name='softmax')      \n",
    "    \n",
    "    variable_averages = tf.train.ExponentialMovingAverage(\n",
    "        inception_model.MOVING_AVERAGE_DECAY)\n",
    "    variables_to_restore = variable_averages.variables_to_restore()\n",
    "    saver = tf.train.Saver(variables_to_restore)    \n",
    "    \n",
    "    # Load checkpoint\n",
    "    ckpt = tf.train.get_checkpoint_state(FLAGS.checkpoint_dir)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        if os.path.isabs(ckpt.model_checkpoint_path):\n",
    "            # Restores from checkpoint with absolute path.\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        else:\n",
    "            # Restores from checkpoint with relative path.\n",
    "            saver.restore(sess, os.path.join(FLAGS.checkpoint_dir,\n",
    "                                             ckpt.model_checkpoint_path))\n",
    "\n",
    "        # Assuming model_checkpoint_path looks something like:\n",
    "        #   /my-favorite-path/imagenet_train/model.ckpt-0,\n",
    "        # extract global_step from it.\n",
    "        global_step = ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1]\n",
    "        print('Succesfully loaded model from %s at step=%s.' %\n",
    "            (ckpt.model_checkpoint_path, global_step))\n",
    "    else:\n",
    "        print('No checkpoint file found')\n",
    "        \n",
    "    graph_def = sess.graph.as_graph_def()\n",
    "    tf.train.write_graph(sess.graph.as_graph_def(), FLAGS.export_dir, FLAGS.graph_name)\n",
    "    \n",
    "    print('Successfully converted checkpoint %s into proto %s/%s with inputs of size %d' % \n",
    "         (FLAGS.checkpoint_dir, FLAGS.export_dir, FLAGS.graph_name, FLAGS.image_size))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'save/Assign_385', u'save/restore_slice_386/tensor_name', u'save/restore_slice_386/shape_and_slice', u'save/restore_slice_386', u'save/Assign_386', u'save/restore_slice_387/tensor_name', u'save/restore_slice_387/shape_and_slice', u'save/restore_slice_387', u'save/Assign_387', u'save/restore_all']\n",
      "[u'input', u'Squeeze', u'DecodeJpeg', u'convert_image/Cast', u'convert_image/y', u'convert_image', u'Shape', u'assert_positive/Const', u'assert_positive/assert_less/Less', u'assert_positive/assert_less/Const']\n"
     ]
    }
   ],
   "source": [
    "names = [node.name for node in graph_def.node]\n",
    "print names[-10:]\n",
    "print names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    g = tf.GraphDef()\n",
    "    with open('/media/esteva/ExtraDrive1/ThrunResearch/tf_experiments/nine-way/export/skin_graph.pb') as f:\n",
    "        g.ParseFromString(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'inception_v3/logits/pool/AvgPool',\n",
       " u'inception_v3/logits/flatten/Reshape/shape',\n",
       " u'inception_v3/logits/flatten/Reshape',\n",
       " u'logits/logits/weights',\n",
       " u'logits/logits/weights/read',\n",
       " u'logits/logits/biases',\n",
       " u'logits/logits/biases/read',\n",
       " u'inception_v3/logits/logits/xw_plus_b/MatMul',\n",
       " u'inception_v3/logits/logits/xw_plus_b',\n",
       " u'softmax']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nombres = [node.name for node in g.node]\n",
    "nombres[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'input',\n",
       " u'Squeeze',\n",
       " u'DecodeJpeg',\n",
       " u'convert_image/Cast',\n",
       " u'convert_image/y',\n",
       " u'convert_image',\n",
       " u'Shape_1',\n",
       " u'Slice/begin',\n",
       " u'Slice/size',\n",
       " u'Slice']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nombres[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
