{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connected Components Partition\n",
    " - Partitioning Strategy: Recursive Dividing N=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import lib\n",
    "from lib.taxonomy.utils import SynonymsList\n",
    "from lib.notebooks.vis_utils import tic, toc\n",
    "from lib.taxonomy.loading import getEntryValues, gatherSynset, gatherPathsAndLabels, rootNode, rootNodeClasses\n",
    "from lib.taxonomy.loading import setEntries, getEntries\n",
    "from lib.taxonomy.loading import imageExists, Field2meta\n",
    "from lib.taxonomy.loading import TRAINING_SET, TESTING_SET, NO_SET, VALIDATION_SET\n",
    "from lib.taxonomy.graph_structure import Taxonomy, recursive_division\n",
    "from lib.taxonomy.edge_extraction import partition_connected_components\n",
    "from lib.taxonomy.io import generate_symlinks, make_directory_structure, create_symlinks\n",
    "from lib.taxonomy.io import print_partition_statistics\n",
    "\n",
    "import scipy.sparse as sp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_directory = '/ssd/esteva/skindata4/images/'\n",
    "meta_file = '/ssd/esteva/skindata4/meta.json'\n",
    "\n",
    "train_dir = '/ssd/esteva/skindata4/splits/recursive_dividing_N=1000/train'\n",
    "test_dir = '/ssd/esteva/skindata4/splits/recursive_dividing_N=1000/test'\n",
    "labels_file = '/ssd/esteva/skindata4/splits/recursive_dividing_N=1000/labels.txt'\n",
    "\n",
    "skin_prob = 0.4 \n",
    "tax_path_score = 0.8 \n",
    "N=1000\n",
    "\n",
    "curated_test_file = '/ssd/esteva/skindata4/test_sets/validation_set.txt'\n",
    "\n",
    "# Files with entries of the form [path/to/image] [label]\n",
    "# All basenames listed in excluded_datasets will be ommitted from train/val\n",
    "excluded_datasets = [ \n",
    "        '/ssd/esteva/skindata4/test_sets/dermoscopy_test.txt',\n",
    "        '/ssd/esteva/skindata4/test_sets/epidermal_test.txt',\n",
    "        '/ssd/esteva/skindata4/test_sets/melanocytic_test.txt'\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FUNC: insert_datetime_field] Skipping 172 entries that could not load datetime\n",
      "46288 Entries have the datetime metadata\n",
      "Calculating time-camera edge matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/numpy/core/fromnumeric.py:2652: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.\n",
      "  VisibleDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000 / 43591 Elapsed Time: 5.61479592323 Time Remaining: 0.503915336227 Elapsed Time:  596.342364073\n",
      "Adding 43591 edges to the graph\n",
      "Adding 5183 turk edges to the graph\n",
      "Adding 2647 turk edges to the graph\n",
      "Adding 17204 dermquest edges to the graph\n",
      "Adding 21434 edges to the graph based on identical filenames\n",
      "We find 250181 connected components\n",
      "Proposing test set from duplicate_urls_turk2.json\n",
      "Proposed Test Set has 20958 entries\n",
      "Partitioned Test Set has 19644 meta entries\n"
     ]
    }
   ],
   "source": [
    "# We load in images that exist on our filesystem,\n",
    "meta = json.load(open(meta_file))\n",
    "meta = [m for m in meta if imageExists(m, dataset_directory)]\n",
    "\n",
    "# Connected components partition assigns one of TRAINING_SET or TESTING_SET to field 'set_identifier'\n",
    "partition_connected_components(meta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "isic = getEntries(meta, 'database', 'isic')\n",
    "isic = [i for i in isic if 'label' in i and i['label'] in ['benign', 'malignant']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Keep meta with desired skin probs and tax path scores\n",
    "meta = [m for m in meta if 'tax_path_score' in m and m['tax_path_score'] >= tax_path_score]\n",
    "meta = [m for m in meta if m['tax_path']]\n",
    "meta = [m for m in meta if 'skin_prob' in m and m['skin_prob'] >= skin_prob]\n",
    "meta.extend(isic)\n",
    "meta = [m for m in meta if m['set_identifier'] in [TRAINING_SET, TESTING_SET]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fix the naming convention issues of the top 9 categories (to dermal-tumor-benign, etc.)\n",
    "syns = SynonymsList()\n",
    "for m in meta:\n",
    "    rootname = '-'.join(m['tax_path'][0])\n",
    "    rootrename = syns.synonymOf(rootname).split('-')\n",
    "    m['tax_path'][0] = rootrename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept Meta Entries: 146138\n",
      "Initializing Taxonomy\n",
      "Creating vertices...\n",
      "Distributing metadata entries...\n",
      "Initializing vertex variables...\n",
      "Identifying root nodes...\n",
      "Adding top node...\n",
      "Applying TreeLearning: Recursive Dividing with N=1000\n"
     ]
    }
   ],
   "source": [
    "# Rename 'label' field to 'disease_name'. 'label' will be used for integer labels.\n",
    "for m in meta:\n",
    "    if 'label' in m:\n",
    "        m['disease_name'] = m['label']\n",
    "        m['label'] = None\n",
    "\n",
    "print \"Kept Meta Entries: %d\" % len(meta)\n",
    "\n",
    "# Assign nine-way rootnode classes.\n",
    "classes, labels = rootNodeClasses(meta)\n",
    "setEntries(meta, 'label', labels)\n",
    "setEntries(meta, 'clinical_label', labels)\n",
    "\n",
    "meta_train = getEntries(meta, 'set_identifier', TRAINING_SET)\n",
    "meta_test = getEntries(meta, 'set_identifier', TESTING_SET)\n",
    "\n",
    "# Reassign to the training set new labels based on a recursive dividing treelearning partition.\n",
    "taxonomy = Taxonomy(meta_train)\n",
    "new_classes, new_names = recursive_division(taxonomy.top_node, N)\n",
    "sort_indices = np.argsort(new_names)\n",
    "new_classes = [new_classes[i] for i in sort_indices]\n",
    "new_names = [new_names[i] for i in sort_indices]\n",
    "for i, (new_class, new_name) in enumerate(zip(new_classes, new_names)):\n",
    "    new_name = new_name.strip('/').replace('/', '_')\n",
    "    for entry in new_class:\n",
    "        entry['label'] = i\n",
    "        entry['label_name'] = new_name\n",
    "print 'Applying TreeLearning: Recursive Dividing with N=%d' % N\n",
    "\n",
    "def collectSynset(meta_train):\n",
    "    \"\"\"Returns the synset and checks that it is sorted.\n",
    "\n",
    "    Args:\n",
    "        meta_train (list): list of dicts in skindata format. Must contain field 'label' and 'label_name'\n",
    "\n",
    "    Returns\n",
    "        Sorted list of class names in the format [label_name] [label].\n",
    "    \"\"\"\n",
    "    synset = []\n",
    "    for m in meta_train:\n",
    "        synset.append([m['label_name'], m['label']])\n",
    "    synset = {tuple(s) for s in synset}\n",
    "    synset = [list(s) for s in synset]\n",
    "    synset.sort(key=lambda x: x[0])\n",
    "    synset = [[str(ss) for ss in s] for s in synset]\n",
    "    synset = [\" \".join(s) for s in synset]\n",
    "\n",
    "    # run sort checks\n",
    "    ss = np.sort(synset)\n",
    "    for i,j in zip(ss, synset):\n",
    "        assert i == j\n",
    "\n",
    "    for i, j in zip([s.split()[1] for s in ss], [s.split()[1] for s in synset]):\n",
    "        assert i == j\n",
    "\n",
    "    return synset\n",
    "\n",
    "\n",
    "synset = collectSynset(meta_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of meta_train 128038\n",
      "Size of meta_test 18100\n",
      "Keeping test set images that have been manually curated. Using curated test file: /ssd/esteva/skindata4/test_sets/validation_set.txt\n",
      "14839\n"
     ]
    }
   ],
   "source": [
    "print 'Size of meta_train %d' % len(meta_train)\n",
    "print 'Size of meta_test %d' % len(meta_test)\n",
    "\n",
    "# Keep only the test set entries that have passed manual curation\n",
    "print 'Keeping test set images that have been manually curated.',\n",
    "print 'Using curated test file: %s' % curated_test_file\n",
    "curated_test = [line.strip() for line in\n",
    "                open(curated_test_file).readlines()]\n",
    "curated_test = np.array([os.path.basename(t.split()[0]) for t in curated_test])\n",
    "\n",
    "filename2meta = Field2meta(meta_test, field='filename')\n",
    "for fn in curated_test:\n",
    "    ms = filename2meta(fn)\n",
    "    for m in ms:\n",
    "        m['cc_keep'] = True\n",
    "\n",
    "for m in meta_test:\n",
    "    if 'cc_keep' not in m:\n",
    "        m['set_identifier'] = NO_SET\n",
    "\n",
    "# Exclude all specified datasets\n",
    "for exclusion_file in excluded_datasets:\n",
    "    filenames = [os.path.basename(line.strip().split()[0]) for line in open(exclusion_file).readlines()]\n",
    "\n",
    "    for fn in filenames:\n",
    "        ms = filename2meta(fn)\n",
    "        for m in ms:\n",
    "            m['set_identifier'] = NO_SET\n",
    "\n",
    "meta_test = getEntries(meta, 'set_identifier', TESTING_SET)\n",
    "print len(meta_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering paths and labels from the metadata\n",
      "Train and test share 0 images, according to filenames\n",
      "Train and val share 0 images, according to filenames\n",
      "Test and val share 0 images, according to filenames\n",
      "Dataset sizes (Based on Metadata):\n",
      "Train,\tVal,\tTest,\tTotal\n",
      "1036 \t0 \t118 \t1154\n",
      "7919 \t0 \t872 \t8791\n",
      "950 \t0 \t76 \t1026\n",
      "4567 \t0 \t641 \t5208\n",
      "8808 \t0 \t1461 \t10269\n",
      "5085 \t0 \t368 \t5453\n",
      "81907 \t0 \t10740 \t92647\n",
      "13088 \t0 \t380 \t13468\n",
      "4678 \t0 \t183 \t4861\n",
      "\n",
      "128038 0 14839\n",
      "\n",
      "Dataset sizes (Based on unique images):\n",
      "Train,\tVal,\tTest,\tTotal\n",
      "978 \t0 \t117 \t1095\n",
      "7618 \t0 \t858 \t8476\n",
      "925 \t0 \t76 \t1001\n",
      "4426 \t0 \t635 \t5061\n",
      "8443 \t0 \t1449 \t9892\n",
      "4959 \t0 \t366 \t5325\n",
      "77165 \t0 \t10666 \t87831\n",
      "13010 \t0 \t375 \t13385\n",
      "4595 \t0 \t170 \t4765\n",
      "# Unique Images in Training: 123162\n",
      "# Unique Images in Validation: 0\n",
      "# Unique Images in Testing: 14712\n",
      "\n",
      "Directory created: /ssd/esteva/skindata4/splits/recursive_dividing_N=1000/train\n",
      "Directory created: /ssd/esteva/skindata4/splits/recursive_dividing_N=1000/test\n",
      "Labels file created: /ssd/esteva/skindata4/splits/recursive_dividing_N=1000/labels.txt\n"
     ]
    }
   ],
   "source": [
    "print 'Gathering paths and labels from the metadata'\n",
    "trainset = np.unique(gatherPathsAndLabels(meta, dataset_directory, TRAINING_SET))\n",
    "valset = np.unique(gatherPathsAndLabels(meta, dataset_directory, VALIDATION_SET))\n",
    "testset = np.unique(gatherPathsAndLabels(meta, dataset_directory, TESTING_SET))\n",
    "no_set = np.unique(gatherPathsAndLabels(meta, dataset_directory, NO_SET))\n",
    "\n",
    "print_partition_statistics(meta, classes, dataset_directory)\n",
    "\n",
    "# Make testing directory structure - rootnode classes\n",
    "subclasses = np.unique([s.split()[0].split('_')[0] for s in synset])\n",
    "make_directory_structure(test_dir, subclasses)\n",
    "syms_test = generate_symlinks(testset, test_dir, subclasses)\n",
    "create_symlinks(syms_test)\n",
    "\n",
    "\n",
    "# Make training directory structure - taxonomy training classes\n",
    "subclasses = np.unique([s.replace(' ', '_') for s in synset])\n",
    "make_directory_structure(train_dir, subclasses)\n",
    "syms_train = generate_symlinks(trainset, train_dir, subclasses)\n",
    "create_symlinks(syms_train)\n",
    "\n",
    "print 'Directory created: %s' % train_dir\n",
    "print 'Directory created: %s' % test_dir\n",
    "\n",
    "with open(labels_file, 'w') as f:\n",
    "    prefix = \"\"\n",
    "    for s in subclasses:\n",
    "        f.write(prefix)\n",
    "        f.write(s)\n",
    "        prefix = \"\\n\"\n",
    "print 'Labels file created: %s' % labels_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
